{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP_Model_to_filter_spam_sms\n",
    "***NLP is a branch of artificial intelligence which is focused on the enabling the computers to understand and interpret the human language. The problem with interpreting the human language is that it is not in the a language or state where a computer can understand. Some popular ways of applying nlp are text classification, speech recognition, question answering e.tc***\n",
    "\n",
    "The goal of the project is to classify either a text is a spam or ham from a bunch of messages\n",
    "### 1.Problem identification\n",
    "We receive lots of sms messages on our phone most of the are unwanted, most are just sent for advertisement purposes from the popular telcos we know around. Most email spam messages are commercial in nature. Whether commercial or not, many are not only annoying, but also dangerous because they may contain links that lead to phishing web sites. \n",
    "\n",
    "### 1.1 Expected outcome \n",
    "We are mainly tackling the nuisance of receiving unwanted sms on phone. The messages here are considered spam or ham\n",
    "Spam - unwanted / dangerous message\n",
    "ham - useful\n",
    "\n",
    "### 1.2 Main Goal\n",
    "Thus, the goal is to classify whether message is spam or ham and predict the recurrence and non-recurrence of spam text after a certain period. To achieve this we have used machine learning classification methods to fit a function that can predict the discrete class of new input.\n",
    "\n",
    "### 1.3 Identify Data sources\n",
    "The sms_spam datasets is available on kaggle.com \n",
    "The dataset contains 5559 occurances of spam and ham messages. \n",
    "The dataset has on two columns, one being type and text.\n",
    "\n",
    "#### Getting Stated : Loading essential libraries for data importing and manipulating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset and performing basic data checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms=pd.read_csv(\"C:\\\\Users\\\\Person\\\\Documents\\\\GitHub\\\\phone_sms_filter\\\\sms_spam.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>spam</td>\n",
       "      <td>complimentary 4 STAR Ibiza Holiday or £10,000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>spam</td>\n",
       "      <td>okmail: Dear Dave this is your final notice to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                               text\n",
       "0   ham  Hope you are having a good week. Just checking in\n",
       "1   ham                            K..give back my thanks.\n",
       "2   ham        Am also doing in cbe only. But have to pay.\n",
       "3  spam  complimentary 4 STAR Ibiza Holiday or £10,000 ...\n",
       "4  spam  okmail: Dear Dave this is your final notice to..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5559, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5559</td>\n",
       "      <td>5559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>2</td>\n",
       "      <td>5156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>4812</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        type                    text\n",
       "count   5559                    5559\n",
       "unique     2                    5156\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4812                      30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5559 entries, 0 to 5558\n",
      "Data columns (total 2 columns):\n",
      "type    5559 non-null object\n",
      "text    5559 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.0+ KB\n"
     ]
    }
   ],
   "source": [
    "sms.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.type.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This shows that the unique classifiers are ham and spam***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Feature Engineering\n",
    "***We are simply creating new features from the existing ones.This helps us better understand the dataset, hence will improve model performance***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms['length']=sms['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Hope you are having a good week. Just checking in</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>K..give back my thanks.</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ham</td>\n",
       "      <td>Am also doing in cbe only. But have to pay.</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  type                                               text  length\n",
       "0  ham  Hope you are having a good week. Just checking in      49\n",
       "1  ham                            K..give back my thanks.      23\n",
       "2  ham        Am also doing in cbe only. But have to pay.      43"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***So a new column length has been entered and there are 3 columns now.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Exploratory Data Analysis \n",
    "We will be taking a closer look at the attributes of the data and try to draw some insights from it.\n",
    "\n",
    "### 3.1 Objectives of Data Exploration\n",
    "* To gain maximum insights about the underlying structure.\n",
    "* Helps in the data preprocessing step , due to the overall picture the analysis gives.\n",
    "\n",
    "We will perform two tasks at this stage:\n",
    "* Descriptive Statistics \n",
    "It is the process of changing key characteristics of the data set into simple numeric metrics. These metrics from mean, standard deviation, and correlation. \n",
    "* Visualizing\n",
    "Helps to project the data into abstract images. These come in the form of graphs, barcharts, pie charts etc.\n",
    "\n",
    "\n",
    "### Basic Descriptive Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>5559.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>79.781436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>59.105497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>61.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>121.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>910.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            length\n",
       "count  5559.000000\n",
       "mean     79.781436\n",
       "std      59.105497\n",
       "min       2.000000\n",
       "25%      35.000000\n",
       "50%      61.000000\n",
       "75%     121.000000\n",
       "max     910.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843     Ok\n",
       "1897    Ok\n",
       "2348    Ok\n",
       "3821    Ok\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms['length'] == 2][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OK** is the text with the minimum text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms['length'] == 910][\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to have a fell of the text with maximum length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We can infer that, the maximum text length is 910 and the minimum being 2***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4812\n",
       "spam     747\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***This is showing the number of ham and spam instances*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "length    2.42243\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***At this stage we will visualize the lengths of the texts***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x278e4c65988>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.countplot(x=\"type\" ,data=sms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***We have visualy seen the number of ham and spam messages with the helps of the seaborn library.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the distribution of data\n",
    "***The main aim here is to visualize the occurence of ham and spam messages.\n",
    "We will visualize the distribution of message lengths.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x278e66020c8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x648 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sms['length'].plot.hist(bins=70,figsize=(7,9),grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***From the histogram, it shows that the lengthest texts have the least occurance and the shorter messages have the most\n",
    "occurence.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities/semantics. The process of converting words into numbers are called ***Vectorization***\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Punctuation from the characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding out how the functions work before applying to the whole dataframe.\n",
    "\n",
    "***Using the index 10 text for the testing***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial=sms['text'].iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing punctuation for selected text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_punc=[a for a in trial if a not in string.punctuation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no_punc=''.join(no_punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making the text a list\n",
    "#no_punc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords from the text.\n",
    "***Stopwords are the words in any language which does not add much meaning to a sentence. \n",
    "They can safely be ignored without sacrificing the meaning of the sentence.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_trial=[word for word in no_punc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***At this point all stopwords have been removed***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So we want to apply this to the whole dataframe\n",
    "***We try to write a function that can take the dataframe through removing punctuation and removing stopwords***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(sms_texts):\n",
    "    \"\"\"\n",
    "    * Remove punctuations\n",
    "    * Remove stopwords\n",
    "    * Return list of clean_words(after removing stopwords)\n",
    "    \"\"\"\n",
    "    no_punctuation=[char for char in sms_texts if char is not string.punctuation]\n",
    "    no_punctuation=''.join(no_punctuation)\n",
    "    return [word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Running the function on the whole dataframe.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model Pipelining feature\n",
    "The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated.\n",
    "The pipeline feature makes our work much easier.\n",
    "***Combines CountVectorizer() , TfidTransformer() , MultinomialNB() or any other algorithm for training***\n",
    "\n",
    "its okay to build your own pipelines but this sklearn feature makes the work much easier.\n",
    "We will be :\n",
    "* Split the data into train and test set\n",
    "* Initialize the pipeline\n",
    "* Take the model through countvectorizer\n",
    "* Take the model through TfidTransformer\n",
    "* BUild the nlp model on MultinimialNB.\n",
    "* Evaluate the model\n",
    " \n",
    " CountVectorizer - provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. \n",
    " Tokenize text - basically refers to splitting up a larger body of text into smaller lines, words or even creating words for a non-English language. \n",
    " TfidTransformer - The goal of using tf-idf is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence give less informative than features that occur in a small fraction of the training corpus.\n",
    " MultinomialNB- the model will be built on naive bayes classifier.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=sms.drop(['type','length'],axis=1)\n",
    "target=sms.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train,features_test,target_train,target_test=train_test_split(sms['text'],sms['type'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the pipeline sklearn feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline=Pipeline([\n",
    "    (\"trial\",CountVectorizer(analyzer=text_processing)),\n",
    "    (\"tfid\",TfidfTransformer()),\n",
    "    (\"classifier\",MultinomialNB())\n",
    "]\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('trial',\n",
       "                 CountVectorizer(analyzer=<function text_processing at 0x00000278E4206678>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfid',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(features_train,target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Basically, the pipeline just took us through CountVectorizer,TfidfTransformer in the matter of seconds. ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "At this stage we will run the model on the test set, and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pipeline.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.97      1013\n",
      "        spam       0.66      1.00      0.80        99\n",
      "\n",
      "    accuracy                           0.96      1112\n",
      "   macro avg       0.83      0.98      0.89      1112\n",
      "weighted avg       0.97      0.96      0.96      1112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
